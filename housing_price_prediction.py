# -*- coding: utf-8 -*-
"""Enhancing 6100_Housing_Price_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zn69CY04FwI2gJ71dFQzoPmJ5SFreQV-

### House Price Prediction
### Student Name: Ridwan Adeniyi
### Student Number: 1366306
### Assignment: Ames housing dataset

This project aimed to predict the prices of unseen houses with the aid of a **linear regression model** by learning from a known dataset containing different house features and known prices.

I will be importing different libraries, such as pandas, which helps with data manipulation and analysis, numpy for numerical operations, matplotlib for plotting modules, and seaborn for aided data visualization, to help with the data preprocessing.
"""

# Importing import libraries for manipulation and analysis
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme()

"""##### Reading the required dataset to be analysed from the system's memory."""

# Loading the dataset
training_data = pd.read_csv('/Housing_Data_Train.csv')
test_data =pd.read_csv('/Housing_Data_Test.csv')
print(training_data.head(10))

# To check the number of columns and rows of the dataset
print(training_data.shape)

"""# **Data Pre-processing**

## Data Cleaning- Null values and Duplicates

To drop the Id and the Unnamed: 0 columns as they are not useful variables for the dataset.
"""

# To drop the Id and Unnamed:0 columns
training_data = training_data.drop(['Id', 'Unnamed: 0'], axis=1)

# To get a better description of my dataset
print(training_data.describe().T)

# To check detailed infomation of the dataset, which shows the columns with null values
print(training_data.info())

# To check a visual heatmap of the null values
sns.heatmap(training_data.isnull(), yticklabels = False , cbar = False)

# To create a list of the columns to show the sum of the missing values in the individual columns

training_data_dict = list(zip(training_data.columns, training_data.isnull().sum()))
print(training_data_dict)

(training_data['MasVnrType'] == 'None').sum()

# None in the MasVnrType are being regarded as null values which continues to mess with the cleaning of this dataset.

training_data['MasVnrType'] = training_data['MasVnrType'].astype(object).where(training_data['MasVnrType'].notna(), 'None')

"""#####  The columns with high numbers of missing values are paid close attention"""

# To check number of unique features and their null values
for col in ['PoolQC','Fence','MiscFeature','FireplaceQu','MasVnrType', 'Alley','LotFrontage']:
  print(f'No. of unique features in {col}: {len(training_data[col].unique())}',
        '\n',
        f'No of NaN values in {col}: {training_data[col].isna().sum()}' '\n')

"""### Exploring Pool Quality and Pool Area"""

training_data['PoolQC'].value_counts()

training_data['PoolQC'].isnull().sum()

training_data['PoolArea'].value_counts()

"""From the analysis, the null values in the pool quality [PoolQC] is same as the pools that have 0 sqft area in [PoolArea]. Therefore, the missing values in PoolQC can be filled with NA as it means No Pool in the data description."""

# Filling the null values in PoolQC with NA
training_data.fillna({'PoolQC': 'NA'}, inplace=True)

"""### Exploring Fireplaces and FireplaceQC"""

training_data['FireplaceQu'].value_counts()

training_data['FireplaceQu'].isnull().sum()

training_data['Fireplaces'].value_counts()

"""From the analysis, the null values in the Fireplace quality [FireplaceQu] is same as the houses with no or 0 Fireplaces [Fireplaces]. Therefore, the missing values in FireplaceQu can be filled with NA as it means No Fireplace in the data description."""

# Filling the null values in FireplaceQu with NA
training_data.fillna({'FireplaceQu' : 'NA'}, inplace = True)

training_data.isnull().sum().sort_values(ascending=False).head(20)

"""######  The remaining columns with missing data are LotFrontage, GarageCond, GarageFinish, GarageType, GarageQual, GarageYrBlt, BsmtFinType2, BsmtExposure, BsmtQual, BsmtFinType1, BsmtCond, MasVnrArea, Fence, MiscFeature, Alley

### Exploring Garage Related Features

#####  The null values in the garage related features which are 'GarageCond', 'GarageFinish', 'GarageType', 'GarageQual', 'GarageYrBlt are all 61 which gives some insight to this. There is a common information missing in all which might be because there are no garages in these houses. However, further analysis would be done to check this.
"""

# Creating a mini dataset containing only the garage features
training_data_garage = training_data[[
    'GarageCond',
    'GarageFinish',
    'GarageType',
    'GarageCars',
    'GarageArea',
    'GarageQual',
    'GarageYrBlt',
]]

training_data_garage.isnull().sum()

# Plotting an heatmap to see the position of individual entries on the dataset
sns.heatmap(training_data_garage.isnull(), yticklabels = False, cbar = False)

"""#####  This plot suggests that the missing values in these set of columns comes from same set of entries. This is logical since a garage that does not exist would not have been built; therefore, not having Garage Year Built.

#####  With this information, i will be filling the missing values with NA and 0 as required.
"""

training_data = training_data.fillna({'GarageCond' : 'NA', 'GarageFinish' : 'NA', 'GarageType' : 'NA', 'GarageQual' : 'NA', 'GarageYrBlt' : 0})

training_data.info()

"""######  The remaining columns with missing data are LotFrontage, BsmtFinType2, BsmtExposure, BsmtQual, BsmtFinType1, BsmtCond, MasVnrArea, Fence, MiscFeature, Alley"""

training_data[['LotFrontage', 'BsmtFinType2', 'BsmtExposure',
               'BsmtQual', 'BsmtFinType1', 'BsmtCond', 'MasVnrArea']].isnull().sum()

training_data[['BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2','BsmtFinSF2']] [training_data['BsmtFinType1'].isnull() & training_data['BsmtFinType2'].isnull()]

"""##### From the data description, the BsmtFinSF1 means Type 1 finished basement square feet while BsmtFinType1, which has missing values, is the Rating of basement finished area. From the code above, the basement square feet of the missing parts of the Rating of basement finished area states 0, which therefore means 'No basement'. Therefore, I'd be filling the empty cells with NA. Vice versa for the BsmtFinType2, BsmtExposure, BsmtQual, BsmtCond."""

training_data = training_data.fillna({'BsmtFinType1' : 'NA','BsmtFinType2' : 'NA','BsmtExposure' : 'NA', 'BsmtQual' : 'NA', 'BsmtCond' : 'NA'})

"""###### The remaining columns with missing data are MasVnrArea, Fence, MiscFeature, Alley, and LotFrontage.

### Masonry Veneer Features
"""

# To check the MasVnrArea with null values and match with MasVnrType to see the relationship

training_data[['MasVnrArea', 'MasVnrType']] [training_data['MasVnrArea'].isnull()]

"""#####  The above output shows that all the null values are as a result of the Masonry veneer type being 'None' which means no Masonry veneer; therefore, the area would definately be 0 as it does not exist."""

training_data.fillna({'MasVnrArea' : 0}, inplace = True)

training_data.info()

"""###  Exploring Electrical"""

training_data['Electrical'].value_counts()

# To check the year in which the house with the missing electrical value was built,
# because this may have same correlation to which electrical system was used during that timeline for electrical constructions.

training_data['YearBuilt'][training_data['Electrical'].isnull()]

# To check the common electricals in 2006
training_data['Electrical'][training_data['YearBuilt'] == 2006].value_counts()

"""#####  From the analysis above, we could see that the most common electrical in 2006 is SBrkr; therefore, i'd fill the missing value of the Electrical column with SBrkr."""

training_data.fillna({'Electrical':'SBrkr'}, inplace = True)

"""### Columns with majority null values

Due to the large chunk of these columns being missing values, i will be dropping, the Fence, MiscFeature, Alley columns as they are the only columns left with high missing values.
"""

# Drop the Fence, MiscFeature, and Alley columns
training_data.drop(['Fence', 'MiscFeature', 'Alley'], axis=1, inplace=True)

"""###  Now i am left with only LotFrontage has the only column with missing data. I would like to fill this missing values with the best acting process between the mean and median.
#####  I would like to use the SimpleImputer from Sklearn for this. Suggested by ChatGPT for faster computation.
"""

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression

#  Naming this variable X_clean to differentiate it from the forthcoming X_test.
X_clean = training_data.drop('SalePrice', axis = 1)
numeric_cols = X_clean.select_dtypes(include=['int64', 'float64']).columns
X_clean_num = X_clean[numeric_cols]
y_clean = training_data['SalePrice']

X_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(X_clean_num, y_clean, test_size = 0.2, random_state = 42)

"""#### Mean and Median Imputation Pipeline"""

mean_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
  ('regressor', LinearRegression())
])



median_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('regressor', LinearRegression())
])

"""####  Training both pilpelines"""

mean_pipeline.fit(X_train_clean, y_train_clean)
median_pipeline.fit(X_train_clean, y_train_clean)

mean_pred = mean_pipeline.predict(X_test_clean)
median_pred = median_pipeline.predict(X_test_clean)

#  To get the simple square loss (Predicted Sale Price - Actual Sale Price)^2
mean_ssl = ((mean_pred - y_test_clean)**2).sum()
median_ssl = ((median_pred - y_test_clean)**2).sum()

print ('mean square loss: ', mean_ssl)
print ('median square loss: ', median_ssl)

"""#####  From the simple square loss, it is obvious that the median gives a better error; therefore, i'd use the median to fill the missing values in 'LotFrontage' column."""

training_data.fillna({'LotFrontage' : training_data['LotFrontage'].median()}, inplace = True)

training_data.info()

"""##### I will be removing outliers from the SalePrice, LotArea, GrLivArea, and TotalBsmtSF columns."""

# Select only numeric columns
key_columns = ['SalePrice', 'LotArea', 'GrLivArea', 'TotalBsmtSF']

# Define function to remove outliers
def remove_outliers_iqr(data, columns):
    for col in columns:
        Q1 = data[col].quantile(0.25)
        Q3 = data[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        data = data[(data[col] >= lower_bound) & (data[col] <= upper_bound)]
    return data

# Remove outliers
training_data = remove_outliers_iqr(training_data, key_columns)

"""##### Now the training data set is a clean data, i would like to apply the same wrangling to the test set and eliminate any unclean parts."""

test_data.head(10)

test_data.isnull().sum().sort_values(ascending=False).head(20)

"""##### All the changes would be made to the test set"""

test_data = test_data.drop(['Id', 'Unnamed: 0'], axis=1)
test_data.fillna({'PoolQC': 'NA'}, inplace=True)
test_data.fillna({'FireplaceQu' : 'NA'}, inplace = True)
test_data.drop(['Fence', 'MiscFeature', 'Alley'], axis=1, inplace=True)
test_data = test_data.fillna({'GarageCond' : 'NA', 'GarageFinish' : 'NA', 'GarageType' : 'NA', 'GarageQual' : 'NA', 'GarageYrBlt' : 0})
test_data = test_data.fillna({'BsmtFinType1' : 'NA','BsmtFinType2' : 'NA','BsmtExposure' : 'NA', 'BsmtQual' : 'NA', 'BsmtCond' : 'NA'})
test_data.fillna({'MasVnrArea' : 0}, inplace = True)
test_data['MasVnrType'] =test_data['MasVnrType'].astype(object).where(test_data['MasVnrType'].notna(), 'None')

test_data.isnull().sum().sort_values(ascending=False).head()

test_data.fillna({'LotFrontage' : test_data['LotFrontage'].median()}, inplace =True)

test_data.isnull().sum().sort_values(ascending=False).head()

test_data.info()

test_data.columns

"""The test data also has been cleaned and does not contain any missing values anymore.

### Encoding

##### Encoding the categorical variables

###### Code functions suggested by ChatGPT
"""

# Define column groups and their orders
encoding_orders = {
    # Group 1
    'ExterQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],
    'ExterCond': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],
    'HeatingQC': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],
    'KitchenQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],

    # Group 2
    'BsmtQual': ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],
    'BsmtCond': ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],
    'FireplaceQu': ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],
    'GarageQual': ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],
    'GarageCond': ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],

    # Group 3
    'BsmtExposure': ['NA', 'No', 'Mn', 'Av', 'Gd'],

    # Group 4
    'BsmtFinType1': ['NA', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],
    'BsmtFinType2': ['NA', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],

    # Group 5
    'GarageType': ['NA', 'Detchd', 'CarPort', 'BuiltIn', 'Basment', 'Attchd', '2Types'],

    # Group 6
    'GarageFinish': ['NA', 'Unf', 'RFn', 'Fin'],

    # Group 7
    'PoolQC': ['NA', 'Fa', 'TA', 'Gd', 'Ex'],

    # Group 8
    'LotShape': ['IR3', 'IR2', 'IR1', 'Reg'],

    # Group 9
    'Utilities': ['ELO', 'NoSeWa', 'NoSewr', 'AllPub'],

    # Group 10
    'LandSlope': ['Sev', 'Mod', 'Gtl'],

    # Group 11
    'HouseStyle': ['1Story', '1.5Unf', '1.5Fin', 'SFoyer',
                   'SLvl', '2Story', '2.5Unf', '2.5Fin'],

    # Group 12
    'Functional': ['Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'],

    # Group 13
    'PavedDrive': ['N', 'P', 'Y']

}


# Define the cumulative ordinal encoding function
def cumulative_encode(series, order):
    """Encodes categorical data cumulatively according to given order."""
    matrix = []
    for val in series.fillna('NA'):  # Replace missing values with 'NA' to match order
        if val not in order:
            val = 'NA'  # Handle unexpected categories
        idx = order.index(val)
        encoded = [1 if i <= idx else 0 for i in range(len(order))]
        matrix.append(encoded)
    return pd.DataFrame(matrix, index=series.index, columns=[f"{series.name}_{cat}" for cat in order])


# Apply encoding to all specified columns
encoded_training_data = []

for col, order in encoding_orders.items():
    if col in training_data.columns:
        encoded = cumulative_encode(training_data[col], order)
        encoded_training_data.append(encoded)
    else:
        print(f"Column {col} not found in dataset.")

# Combine encoded columns with original dataset
encoded_training_data = pd.concat(encoded_training_data, axis=1)
cum_training_data = pd.concat([training_data, encoded_training_data], axis=1)

cum_training_data.shape

# Checking the infomation on the cumulatively encoded training dataset
print(cum_training_data.info())

# Identify categorical columns
categorical_cols = ['MSZoning', 'Street', 'LandContour', 'LotConfig',
       'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'RoofStyle',
       'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType','Foundation',
       'Heating', 'Electrical', 'SaleType', 'SaleCondition']

# Apply One-Hot Encoding using pandas
training_cum_onehot = pd.get_dummies(cum_training_data, columns=categorical_cols, drop_first=False)

print("One-Hot Encoding completed and saved as 'Housing_Data_Train_OneHotEncoded.csv'")
training_cum_onehot.head()

"""#####  The last encoding i will be doing will be the Label encoding which would be done on the CentralAir as it is a Yes or No column and i would prefer label encoding as yes would be 1 and No would be 0. I applied label encoding to preserve the ranking information while not increasing the number of variables being created."""

# Define label mappings
centralair_order = {'N': 0, 'Y': 1}

# Apply mapping
training_cum_onehot['CentralAir'] = training_cum_onehot['CentralAir'].map(centralair_order)


print(training_cum_onehot)

# Checking specific columns of this dataset
training_cum_onehot.iloc[:, 9:30].head()

# Dropping the columns that has been encoded by cummulative encoding but the main columns still remained
training_cum_onehot.drop(['ExterQual','ExterCond', 'HeatingQC', 'KitchenQual',
                          'BsmtQual', 'BsmtCond', 'FireplaceQu','GarageQual','GarageCond',
                          'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',
                          'GarageType', 'GarageFinish', 'PoolQC', 'LotShape', 'Utilities',
                          'LandSlope', 'HouseStyle', 'Functional', 'PavedDrive'], axis = 1, inplace=True)

training_cum_onehot.info()

"""##### Would be running same encoding on the test dataset too."""

# Apply encoding to all specified columns using the cumulative_encode function earlier defined
encoded_test_data = []

for col, order in encoding_orders.items():
    if col in test_data.columns:
        encoded_test = cumulative_encode(test_data[col], order)
        encoded_test_data.append(encoded_test)
    else:
        print(f"Column {col} not found in dataset.")

# Combine encoded columns with original dataset
encoded_test_data = pd.concat(encoded_test_data, axis=1)
cum_test_data = pd.concat([test_data, encoded_test_data], axis=1)


# The next line of codes are for the one-hot encoding



# Apply One-Hot Encoding using pandas
test_cum_onehot = pd.get_dummies(cum_test_data, columns=categorical_cols, drop_first=False)


# The label encoding on the CentralAir column.
# Define label mappings

# Apply mapping
test_cum_onehot['CentralAir'] = test_cum_onehot['CentralAir'].map(centralair_order)


print(test_cum_onehot)

test_cum_onehot.drop(['ExterQual','ExterCond', 'HeatingQC', 'KitchenQual',
                          'BsmtQual', 'BsmtCond', 'FireplaceQu','GarageQual','GarageCond',
                          'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',
                          'GarageType', 'GarageFinish', 'PoolQC', 'LotShape', 'Utilities', 'LandSlope', 'HouseStyle', 'Functional', 'PavedDrive'], axis = 1, inplace=True)

training_cum_onehot.columns

test_cum_onehot.columns

"""##### From the encodings, due to the unavailability of some values within the test set columns, some binary columns were not created; therefore, bringing about a difference between the training and test column numbers."""

# To check the columns present in the training dataset but not in the test dataset
set(training_cum_onehot.columns).symmetric_difference(set(test_cum_onehot.columns))

# To include all missing columns in the test dataset from the training dataset
test_cum_onehot = test_cum_onehot.reindex(columns=training_cum_onehot.columns, fill_value=0)
# To drop the SalePrice column as it is not required in the test dataset
test_cum_onehot.drop(['SalePrice'], axis = 1, inplace = True)

test_cum_onehot.info()

training_cum_onehot.isnull()

"""### Feature Selection

##### To get the best performance from a model, the best features has to be used in training the model without inflating it complexity, putting in mind the danger of too few and too many features, while trying to look for the sweet spot.

##### Therefore, a forward subset selection was performed.

##### This entire feature seletion code was heavily borrowed from "https://github.com/mcnica89/DATA6100/blob/main/Variable_Selection/Subset_Selection_Example.ipynb"
"""

from itertools import chain, combinations
import statsmodels.api as sm
from sklearn.metrics import mean_squared_error

training_cum_onehot = training_cum_onehot.astype(float)
X = training_cum_onehot.drop('SalePrice', axis = 1)
y = training_cum_onehot['SalePrice']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

def processSubset(feature_set, X_train, y_train, X_test, y_test):
    # Fit model on feature_set and calculate RSS
    model = sm.OLS(y_train,X_train[list(feature_set)])
    regr = model.fit()
    RMSE = float(np.sqrt(np.mean((regr.predict(X_test[list(feature_set)]) - y_test) ** 2)))
    return {'p':len(feature_set),'features':feature_set, 'model':regr, 'RMSE':RMSE}

def forward(predictors, X_train, y_train, X_test, y_test):

    # Pull out predictors we still need to process
    remaining_predictors = [p for p in X_train.columns if p not in predictors]

    results = []

    for p in remaining_predictors:
        results.append(processSubset(predictors+[p], X_train, y_train, X_test, y_test))

    # Wrap everything up in a nice dataframe
    models = pd.DataFrame(results)

    print("---------")
    for i in range(len(models)):
      print(f" Option # {i}: {models['features'][i]}")
      print(f" RMSE: {round(models['RMSE'][i],1)}")

    # print("----Models:\n", models['features']models['RMSE'])

    # Choose the model with the lowest RMSE
    best_model = models.loc[models['RMSE'].argmin()]

    # Return the best model, along with some other useful information about the model
    return best_model

models = pd.DataFrame(columns=['p','RMSE','features','model'])


predictors = []

for i in range(1,len(X.columns)+1):
    models.loc[i] = forward(predictors, X_train, y_train, X_test, y_test)
    predictors = models.loc[i]['model'].model.exog_names

models['RMSE'] = models['RMSE'].astype(float)

# print results
models

plt.plot(models['p'],models['RMSE'],'ob')
plt.xlabel('#  Predictors')
plt.ylabel('RMSE Test')
plt.plot(models['p'][models['RMSE'].idxmin()], models['RMSE'].min(), 'or')

models[80:100]

"""##### Use the entire training Set (no test set)"""

models.columns

models_no_test = pd.DataFrame(columns=['RMSE', 'model'])

predictors = []

for i in range(1,292):
    models_no_test.loc[i] = forward(predictors, X, y, X, y)
    predictors = models_no_test.loc[i]['model'].model.exog_names

plt.plot(models_no_test['RMSE'],'ob')
plt.xlabel('#  Predictors')
plt.ylabel('RMSE Train')
plt.plot(models_no_test['RMSE'].astype(float).idxmin(), models_no_test['RMSE'].min(), 'or');

"""##### Plotting both the Test and Train RMSE on the same graph to enhance the difference visibility."""

# Plot Test RMSE
plt.plot(models['p'], models['RMSE'], 'o-', color='blue', label='Test RMSE')
plt.plot(models['p'][models['RMSE'].idxmin()], models['RMSE'].min(), 's', color='red', label='Min Test RMSE')

# Plot Train RMSE
plt.plot(models_no_test['RMSE'], 'o-', color='green', label='Train RMSE')
plt.plot([models_no_test['RMSE'].astype(float).idxmin()],
         models_no_test['RMSE'].min(), 'D', color='red', label='Min Train RMSE')

# Label underfitting (first point on Test RMSE)
plt.annotate('Underfitting',
             xy=(models['p'].iloc[0], models['RMSE'].iloc[0]),
             xytext=(models['p'].iloc[0]+0.5, models['RMSE'].iloc[0]+0.5),
             arrowprops=dict(facecolor='black', arrowstyle='->'))

# Label best (minimum Test RMSE)
best_idx = models['RMSE'].idxmin()
plt.annotate('Best',
             xy=(models['p'][best_idx], models['RMSE'].min()),
             xytext=(models['p'][best_idx]-1, models['RMSE'].min()+0.5),
             arrowprops=dict(facecolor='black', arrowstyle='->'))

# Label overfitting (last point on Test RMSE)
plt.annotate('Overfitting',
             xy=(models['p'].iloc[-1], models['RMSE'].iloc[-2]),
             xytext=(models['p'].iloc[-1]-2, models['RMSE'].iloc[-1]+0.5),
             arrowprops=dict(facecolor='black', arrowstyle='->'))

plt.xlabel('# Predictors')
plt.ylabel('RMSE')
plt.title('Train vs Test RMSE vs # Predictors')
plt.legend()
plt.grid(True)
plt.show()

print("Best 91 Variables from our training/test split:")
print(sorted(models.loc[91, 'model'].model.exog_names))

print("----")

print("Best 91 Variables using the entire model (no test set):")
print(sorted(models_no_test.loc[91, 'model'].model.exog_names))

"""#### After getting the best variables to use in the predictions i would be making a model for prediction."""

best_features = models_no_test.loc[91, 'model'].model.exog_names
X_best_train = X[best_features]
y_best = training_cum_onehot['SalePrice']

final_model = LinearRegression()
final_model.fit(X_best_train,y_best) # this fits the model

# Predict on the test set
X_best_test = test_cum_onehot[best_features]
predictions = final_model.predict(X_best_test)

# convert the predictions to a LIST of POSITIVE INTEGERS (do not submit decimals!)
list_predictions = predictions.tolist()
rounded_list_predictions = [int(x) for x in list_predictions]
print(rounded_list_predictions)

# copy paste the list into a function called "my_answer_list()"
# then save this function as "calculator.py" and submit it to Gradescope
# your answers should be integers

def my_answer_list():
  return [219997, 128080, 116288, 135032, 125011, 253012, 64183, 185700, 155210, 161263, 140838, 301821, 271981, 180029, 148426, 425129, 125186, 166172, 216376, 60665, 113460, 189482, 155952, 185942, 124412, 115005, 189310, 206582, 178918, 120771, 120968, 151838, 335190, 237221, 159376, 169136, 106872, 145305, 220122, 212373, 158573, 155316, 202020, 118480, 129156, 107243, 125157, 223787, 275606, 184600, 269100, 149101, 122846, 203375, 169070, 173483, 109386, 192889, 137158, 121415, 92464, 88735, 154805, 115948, 213704, 239913, 187443, 217707, 279630, 80474, 211654, 248688, 90667, 233818, 190095, 204599, 118938, 184050, 126124, 152201, 137824, 185664, 237951, 238102, 129115, 230817, 255877, 149687, 270282, 164337, 177829, 183915, 326306, 146530, 123794, 86895, 122903, 208947, 148275, 105885, 201685, 113842, 295698, 160324, 112486, 114594, 200659, 145258, 253829, 359514, 218450, 201855, 65913, 147528, 103443, 146220, 145454, 209997, 254279, 310898, 253232, 202995, 310321, 148145, 105737, 210622, 181990, 113166, 186117, 125000, 208544, 179923, 241690, 162264, 122887, 198909, 206999, 144242, 360859, 120241, 81653, 179904, 151947, 373523, 163963, 120345, 211294, 158096, 280495, 144663, 221358, 156903, 147113, 161948, 135073, 389398, 235989, 149127, 406093, 116219, 242867, 206826, 127781, 171974, 127217, 135445, 276681, 109156, 105573, 210560, 120497, 280262, 148121, 139572, 128817, 139085, 234875, 116137, 257264, 88321, 396108, 144791, 141799, 107196, 219931, 128963, 123675, 157234, 113125, 138086, 205973, 329684, 153349, 150473, 157740, 76906, 203139, 334540, 197847, 175895, 161904, 126802, 153897, 320508, 109337, 247391, 153076, 158097, 291638, 70614, 137742, 142312, 111182, 293637, 148616, 211165, 225681, 102049, 63575, 109504, 148469, 194036, 274845, 141896, 295329, 199148, 140065, 146739, 222424, 159122, 104840, 256729, 211614, 118720, 130812, 149272, 179229, 151758, 230133, 160336, 272324, 195506, 172841, 148382, 91081, 187872, 226754, 141280, 87300, 212208, 159463, 190301, 158770, 147064, 136022, 161289, 173108, 149184, 144720, 152811, 95878, 184671, 120672, 300372, 206490, 115178, 157161, 133431, 153200, 156618, 164812, 91471, 155460, 125635, 237051, 133539, 100231, 170735, 196542, 164652, 172679, 204869, 215349, 291921, 89456, 198664, 251333, 146529, 136784, 197863, 129842, 138465, 165712, 84374, 98593, 194609, 234072, 212845, 185671, 161933, 151113, 227265, 107982, 168379, 153673, 154982, 285391, 128514, 292081, 230978, 79157, 260116, 324430, 142360, 137810, 194530, 169567, 246412, 165486, 217525, 97338, 197851, 107209, 117044, 123794, 178296, 169720, 193821, 167704, 172326, 164568, 134980, 118808, 38462, 105148, 325880, 132086, 190369, 135991, 230705, 71617, 156030, 205331, 145067, 155780, 223318, 348898, 150781, 142205, 226437, 174779, 224077, 124027, 130801, 153050, 189590, 169972, 179194, 162201, 229829, 117892, 88929, 198127, 265093, 213068, 284403, 166301, 192357, 155629, 142297, 191210, 134415, 155880, 173249, 121128, 207779, 254498, 213024, 97139, 122398, 272390, 161226, 223429, 179873, 174629, 73333, 133510, 128303, 126546, 155613, 311065, 342686, 207959, 155517, 187636, 220321, 110505, 199259, 266547, 92994, 72353, 237169, 107142, 127866, 127152, 232554, 190506, 253235, 261601, 205634, 371889, 188240, 160731, 168604, 191235, 128015, 231929, 176885, 243697, 92757, 63855, 105190, 352671, 123131, 144972, 130922, 187059, 204856, 272515, 119324, 93026, 311272, 197869, 128951, 187974, 185071, 359914, 343783, 100824, 103259, 260492, 173855, 209042, 260428, 137144, 153115]


# The RMSE for this particular example is: 69577

# Note: If your RMSE is "9999999" that means you submitted a list that is the wrong length